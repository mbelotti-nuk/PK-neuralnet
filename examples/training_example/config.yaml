#' Path to input and output files in the training procedure of the Neural Network.
#' In this case the input is the folder in which the database is stored
#' and the output is is the folder in which save the trained neural network.
io_paths:
  #' Database specifications
  #' Path pointing to the folder in which are stored database files
  path_to_database: \absolute\path\to\database
  #' Path in which the folder containing the results will be created
  save_directory: ""



#' Input specifications.
#' Remember that the inputs (or database_inputs if is not None), should be given in the same order as
#' which were saved in the binary files. The order is given in the "Database_specifics.txt" file generated
#' during training.
inp_spec:
  #' Neural network Inputs
  inputs: ['energy', 'dist_source_tally','dist_shield_tally','mfp','theta','fi']

  #' Inputs that are present in the database files.
  #' This input should be specified in case the Neural network Inputs (variable above) 
  #' is a subset of the actual database input set.
  database_inputs: null

  #' Scaling type for each input
  inp_scaletype: {'energy': 'minmax', 'dist_source_tally': 'std', 'dist_shield_tally': 'std', 'mfp': 'std', 'theta': 'std', 'fi': 'std'}

  #' precise spherical harmonic type
  l: 0
  m: 0





#' Ouptut specifications
out_spec:
  #' Neural newtork Output
  output: "Dose"

  #' Output scaling type
  out_scaletype: {'Dose': 'std'}

  #' Clip output to a [min, max]
  out_clip: null  

  #' Use log-scale for training
  out_log_scale: True

  #' Number of voxels in X, Y, Z directions for the reference mesh in the database
  mesh_dim: [80,100,35]

  #' If the database input file contains MCNP errors or not
  errors: true

  #' Clips the training points by selecting just the ones with output (y) between y-3*std(y) and y+3*std(y)
  std_clip: false




#' Neural network specifications
nn_spec:
  #' DNN architecture
  #' a list that gives to each layer the corresponing number of neurons
  #' note that f_maps[0] has to be equal to the number of inputs
  f_maps: [6,128,64,8]

  #' n_files: number of files from database to be used in training. These are randomly selected.
  #' if null, all the files are used
  n_files: 200 #'None

  #' samples_per_case: number of points used for training for each file of the database.
  #' if None, all the points are used which corresponds to mesh_dim[0]*mesh_dim[1]*mesh_dim[2]
  samples_per_case: 100

  #' percentage for training/validation splitting
  percentage: 0.75




#' Training parameters 
training_parameters:
  #' Size of the batch
  batch_size: 2048

  #' Number of epochs in the training
  n_epochs: 1

  #' Number of epochs to wait with no improvements in the loss function
  #' before stopping the training
  patience: 10

  #' Use of mixed precision float 16/32 in GPU training
  #' ref: https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html
  mixed_precision: False

  #' Resamples at each training epochs new training points (the number is equal to "samples_per_case")
  shuffle_train: True




#' Training metrics
metrics:
  #'' Loss function
  loss: "mse"

  #' use modified variance-based mse
  error_loss: true
  #'' Accuracy function
  accuracy: "l1"

#' Optimizer
optimizer:
  type: "adamw"
  learning_rate: 0.001
  weight_decay: 0.001



#' Learning rate scheduler
#' ref: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html
lr_scheduler:
  factor: 0.5 
  patience: 5

